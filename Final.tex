%
%  untitled
%
%  Created by Davis Shepherd on 2012-05-22.
%  Copyright (c) 2012 University of Washington. All rights reserved.
%
\documentclass[twocolumn]{article}

% Use utf-8 encoding for foreign characters
\usepackage[utf8]{inputenc}

% Setup for fullpage use
\usepackage{fullpage}

% Uncomment some of the following if you use the features
%
% Running Headers and footers
%\usepackage{fancyhdr}

% Multipart figures
%\usepackage{subfigure}

% More symbols
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{latexsym}

% Surround parts of graphics with box
\usepackage{boxedminipage}

% Package for including code in the document
\usepackage{listings}

% If you want to generate a toc for each chapter (use with book)
\usepackage{minitoc}

% This is now the recommended way for checking for PDFLaTeX:
\usepackage{ifpdf}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%\pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue
%\fi

\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi
\title{PCA}
\author{ Davis Shepherd \and Jason Uanon \and Yukio Maeda}

\date{\today}

\begin{document}

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi

\maketitle


\begin{abstract}
    This paper aims to explore the utility of using Principal Component Analysis (PCA) in conjunction with a naive bayesian learner.  We look at both computation time and accuracy as a way to measure the performance of such a procedure.
\end{abstract}

\section{Introduction} % (fold)
\label{sec:introduction}
Our STAT 391 Project primarily focuses on the statistical analysis of using off-line pattern classification methods to simplify and interpret any given written number sample. Specifically, the project focuses on the implementation and analysis of using one particular method, merging the concepts of Principal Components Analysis (PCA) and Maximum Likelihood Estimation (MLE) within the context of symbol classification to uniquely identify number samples. Additionally, our project tries to determine the relative success of the given method by comparing the resulting numbers (determined by the final calculated likelihoods) with the expected number as well as comparing the likelihood ratio data with that obtained using other handwriting classifiers. 

The idea of character recognition or handwriting recognition as a topic is mainly due to its increased importance within the digital world. Today, many computerized systems depend on character recognition for basic operation including computer tablet software, postal letter software, and bank check processing equipment. While they do correctly recognize symbols most of the time, it still succumbs to failure, diminishing the usefulness of these systems over a long time period. Findings of better classifiers may lead to the method's implementation and a reduction of bad symbol identifications. In addition, the idea of character recognition was motivated by its application within the field of artificial intelligence. In this case, better implementation of algorithms and distinguishing of alphanumeric features could allow for more adaptable models of automated devices, suited to identify and recognize any alphanumeric character.

Handwriting recognition, or the ability to analyze and correctly identify a particular language representation from handwritten graphical marks, has been the topic of much research within the past thirty years due to its complexity and increasing algorithmic development. In contrast to the success of recognizing symbols by human eye, computer recognition software/hardware has had mixed degrees of success mainly because of the variability of each person's symbol-writing characteristics (e.g. size, position, appearance). As a result of the inconsistency of one classifier, researchers have devised numerous algorithms and methods to classify symbols, with most of them comparing current information with that extracted from pre-captured data to make a logical decision. Examples of this other than the highlighted PCA-analysis include the nearest-neighbor algorithm and greedy-point match algorithm. Also, scientists have recently used real-time factors to better determine subtle characteristics of symbols (e.g. pen velocity, stop patterns), improving the overall process by allowing segmentation of visual elements with gathered temporal information. 
% section introduction (end)
\section{Methods} % (fold)
\label{sec:methods}
Before we begin to talk about principal component analysis (PCA), we review some basic terminology and results from statistics and linear algebra.\\
The \textbf{expected value} of a discrete data set $X$ (also called the mean) is defined as
\begin{center}
$\displaystyle E[X] = \mu =  \sum_{i=1}^{n}x_ip(x_i)$
\end{center}
For the purposes of this data analysis, we will assume that each data point is equally likely. We'll call the expected value 
$\overline{X}$ and write
\begin{center}
$\displaystyle \overline{X} = \frac{\sum_{i=1}^{n}X_i}{n}$
\end{center}
The \textbf{variance} of a data set $X$ is defined as
\begin{center}
$\displaystyle Var(X) = \sigma^2 = E\left[\left(X-\mu\right)^2\right]$
\end{center} 
and using our above definition for the expected value, we have
\begin{center}
$\displaystyle \sigma^2 = 
\frac{\sum_{i=1}^{n}(X_i - \overline{X})^2}{n-1}$
\end{center}
note that the numerator is $n-1$ instead of $n$ for this formula; this is called the \textbf{sample variance} and is used as an unbiased estimator of the true variance of the data.

With this in mind, let $X$ and $Y$ be two identically distributed sets of data. We define the \textbf{covariance} of $X$ and $Y$ as
\begin{center}
$Cov(X, Y) = 
\frac{\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})}{n-1}$
\end{center}
Now, suppose we have $n$ identically distributed random variables $X_1, \ldots, X_n$. We define the \textbf{covariance matrix} $\Sigma$ as $\Sigma_{i, j} = Cov(X_i, X_j)$. Note that this matrix is symmetric, which means that it will always have real-valued eigenvalues. These eigenvectors correspond to how the data are related. Essentially, we convert a set of data into a set of orthogonal vectors (the principal compoenents), which means that they are uncorrelated with each other. By ordering the eigenvectors by eigenvalue from highest to lowest, we obtain the components in order of significance. This can be used to reduce the dimensionality of a data set by only selecting a certain amount of the highest-valued components. For example, if we start out with 100 eigenvectors/eigenvalues and we choose only the first 10, then the final data set will only have 10 dimensions.

Each principal component is formed by taking the values of the elements of the eigenvalues as the weights of the linear combination. That is, if each eigenvector has the form
$${\bf e}_{i} = \left(\begin{array}{c} e_{1i} \\ e_{2i} \\ \vdots \\ e_{ni} \end{array}\right)$$
The the principal components are formed by:
$$Y_1 = {\bf e}_{11}X_1 + {\bf e}_{21}X_2 + \ldots + {\bf e}_{n1}X_n$$
$$Y_2 = {\bf e}_{12}X_1 + {\bf e}_{22}X_2 + \ldots + {\bf e}_{n2}X_n$$
$$\vdots$$
$$Y_n = {\bf e}_{1n}X_1 + {\bf e}_{2n}X_2 + \ldots + {\bf e}_{nn}X_n$$

PCA, then, can be summarized by following these steps:
\begin{itemize}
    \item Obtain training data and subtract the mean
    \item Calculate the covariance matrix
    \item Calculate its eigenvectors and eigenvalues
    \item Choose components and form a feature vector
    \item Derive a new data set
\end{itemize}
For the last bullet point, the new data set can be created by taking the transpose of the feature ector and multiply it on the left of the original data set:
$$FinalData = RowFeatureVector \times RowDataAdjust$$
where $RowFeatureVector$ is the matrix with the eigenvectors in the columns transposed (so tha the eigenvectors are rows, with the most significant on top) and $RowDataAdjust$ is the mean-adjusted data transposed (so the data are in the columns). 

After this procedure, new data can be classified using the new data set. For this project, we will be using a version of the Naive Bayes classifier in addition to PCA to recognize digits. This reduced feature set is used because otherwise every individual pixel of an image would be considered a separate feature; even for small images, this result in a large number of features that would make computations more difficult. Additionally, performing PCA tells us what the most significant components are. By converting these components into images, this can give us more insight into the structure and correlations of each number.

For our purposes, we will simply be using the built-in \emph{prcomp} method in R to perform PCA. Doing ths minimizes the time spent on technical details and allows us to focus more on the classifying itself. Moreover, it also minimizes the risk of having errors in our own code. This background knowledge about what is going on is essential in order to gain insight into what these principal components mean and how to utilize them.

% section methods (end)

\section{Data} % (fold)
\label{sec:data}
The data we will be using is The MNIST Database of Handwritten Digits[1], a freely-available online data set of 60,000 training examples along with a teset set of 10,000 examples. The full data set contains digits written by over 500 different writers, providing a suitable amount of variation for training and testing. Each image is a 28x28 pixel image of a digit, centered and formatted so that it can be read and parsed as an array of numeric values corresponding to the darkness of a pixel. This makes makes it convenient for our purposes, so that we may focus on PCA and classifying instead of gathering data (which may be faulty, mislabeled, damaged, etc.). 
% section data (end)

\section{Training and Testing} % (fold)
\label{sec:training_and_testing}
For the training portion of our experiment, we used Python to parse the image data into an intermediate text file formatted so that it may easily be read by R; letting R parse and read directly seemed to take much longer. From there, the \emph{prcomp} method in R is used to perform PCA to convert the training data into principal components
% section training and testing (end)

\section{Results} % (fold)
\label{sec:results}
% section results (end)

\section{Discussion} % (fold)
\label{sec:discussion}
% section discussion (end)

\section{Bibliography} % (fold)
\label{sec:bibliography}
\begin{enumerate}
\item[[1.]] http://yann.lecun.com/exdb/mnist/
\end{enumerate}
% section bibliography (end)
\bibliographystyle{plain}
\bibliography{}
\end{document}
