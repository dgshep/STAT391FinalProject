%
%  untitled
%
%  Created by Davis Shepherd on 2012-05-22.
%  Copyright (c) 2012 University of Washington. All rights reserved.
%
\documentclass[twocolumn]{article}

% Use utf-8 encoding for foreign characters
\usepackage[utf8]{inputenc}

% Setup for fullpage use
\usepackage{fullpage}

% Uncomment some of the following if you use the features
%
% Running Headers and footers
%\usepackage{fancyhdr}

% Multipart figures
%\usepackage{subfigure}

% More symbols
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{latexsym}

% Surround parts of graphics with box
\usepackage{boxedminipage}

% Package for including code in the document
\usepackage{listings}

% If you want to generate a toc for each chapter (use with book)
\usepackage{minitoc}

% This is now the recommended way for checking for PDFLaTeX:
\usepackage{ifpdf}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%\pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue
%\fi

\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi
\title{PCA}
\author{ Davis Shepherd \and Jason Uanon \and Yukio Maeda}

\date{\today}

\begin{document}

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi

\maketitle


\begin{abstract}
    Our PCA paper on PCA and stuff.
\end{abstract}

\section{Introduction} % (fold)
\label{sec:introduction}
Our STAT 391 Project primarily focuses on the statistical analysis of using off-line pattern classification methods to simplify and interpret any given written number sample. Specifically, the project focuses on the implementation and analysis of using one particular method, merging the concepts of Principal Components Analysis (PCA) and Maximum Likelihood Estimation (MLE) within the context of symbol classification to uniquely identify number samples. Additionally, our project tries to determine the relative success of the given method by comparing the resulting numbers (determined by the final calculated likelihoods) with the expected number as well as comparing the likelihood ratio data with that obtained using other handwriting classifiers. 

The idea of character recognition or handwriting recognition as a topic is mainly due to its increased importance within the digital world. Today, many computerized systems depend on character recognition for basic operation including computer tablet software, postal letter software, and bank check processing equipment. While they do correctly recognize symbols most of the time, it still succumbs to failure, diminishing the usefulness of these systems over a long time period. Findings of better classifiers may lead to the method's implementation and a reduction of bad symbol identifications. In addition, the idea of character recognition was motivated by its application within the field of artificial intelligence. In this case, better implementation of algorithms and distinguishing of alphanumeric features could allow for more adaptable models of automated devices, suited to identify and recognize any alphanumeric character.

Handwriting recognition, or the ability to analyze and correctly identify a particular language representation from handwritten graphical marks, has been the topic of much research within the past thirty years due to its complexity and increasing algorithmic development. In contrast to the success of recognizing symbols by human eye, computer recognition software/hardware has had mixed degrees of success mainly because of the variability of each person's symbol-writing characteristics (e.g. size, position, appearance). As a result of the inconsistency of one classifier, researchers have devised numerous algorithms and methods to classify symbols, with most of them comparing current information with that extracted from pre-captured data to make a logical decision. Examples of this other than the highlighted PCA-analysis include the nearest-neighbor algorithm and greedy-point match algorithm. Also, scientists have recently used real-time factors to better determine subtle characteristics of symbols (e.g. pen velocity, stop patterns), improving the overall process by allowing segmentation of visual elements with gathered temporal information. 
% section introduction (end)
\section{Methods} % (fold)
\label{sec:methods}
Before we begin to talk about principal component analysis (PCA), we review some basic terminology and results from statistics and linear algebra.\\
The \textbf{expected value} of a discrete data set $X$ (also called the mean) is defined as
\begin{center}
$\displaystyle E[X] = \mu =  \sum_{i=1}^{n}x_ip(x_i)$
\end{center}
For the purposes of this data analysis, we will assume that each data point is equally likely. We'll call the expected value 
$\overline{X}$ and write
\begin{center}
$\displaystyle \overline{X} = \frac{\sum_{i=1}^{n}X_i}{n}$
\end{center}
The \textbf{variance} of a data set $X$ is defined as
\begin{center}
$\displaystyle Var(X) = \sigma^2 = E\left[\left(X-\mu\right)^2\right]$
\end{center} 
and using our above definition for the expected value, we have
\begin{center}
$\displaystyle \sigma^2 = 
\frac{\sum_{i=1}^{n}(X_i - \overline{X})^2}{n-1}$
\end{center}
note that the numerator is $n-1$ instead of $n$ for this formula; this is called the \textbf{sample variance} and is used as an unbiased estimator of the true variance of the data.\\
With this in mind, let $X$ and $Y$ be two identically distributed sets of data. We define the \textbf{covariance} of $X$ and $Y$ as
\begin{center}
$Cov(X, Y) = 
\frac{\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})}{n-1}$
\end{center}
Now, suppose we have $m$ identically distributed random variables $X_1, \ldots, X_m$. We define the \textbf{covariance matrix} $\Sigma$ as $\Sigma_{i, j} = Cov(X_i, X_j)$. Note that this matrix is symmetric, which means that it will always have real-valued eigenvalues. These eigenvectors correspond to how the data are related; the eigenvector with the highest eigenvalue is the \emph{principal component} of the data set. By ordering the eigenvectors by eigenvalue from highest to lowest, we obtain the components in order of significance. This can be used to reduce the dimensionality of a data set by only selecting a certain amount of the highest-valued components.\\
PCA, then, can be performed by following these steps:
\begin{itemize}
    \item Obtain data and subtract the mean (normalizing)
    \item Calculate the covariance matrix
    \item Calculate its eigenvectors and eigenvalues
    \item Choose components and form a feature vector
    \item Derive a new data set
\end{itemize}

% section methods (end)
\bibliographystyle{plain}
\bibliography{}
\end{document}
